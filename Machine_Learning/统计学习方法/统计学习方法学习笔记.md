# 统计学习方法笔记



<a id="1"></a>

## Ch01 统计学习方法概论

### 1.1 统计学习

本书主要讨论监督学习

- 假设数据是独立同分布产生的
- 假设要学习的模型属于某个函数集合，称为假设空间（hypothesis space）
- 应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对已知训练数据和未知测试数据在给定的评价准则下有最优的预测

### 1.2 监督学习

**监督学习假设输入与输出的随机变量 $X$ 和 $Y$ 遵循联合概率分布 $P(X,Y).$** 训练数据与测试数据被看做是依联合概率分布 $P(X,Y)$ 独立同分布产生的。统计学习假设数据存在一定的统计规律，$X$ 和 $Y$ 具有联合概率分布的假设就是监督学习关于基本数据的假设。

模型可以是 **概率模型** 或 **非概率模型**，由**条件概率分布** $P(Y|X)$ 或**决策函数** $Y=f(X)$ 表示。

### 1.3 统计学习三要素

#### 1.3.1 模型

假设空间可以定义为 **决策函数** 或 **条件概率** 的集合
$$
\begin{aligned}
& \mathcal{F}=\{f\ |\ Y=f_\theta(X),\quad \theta\in R^n\}           \\
& \mathcal{F}=\{P\ |\ P_\theta(Y|X),\quad \theta\in R^n\}
\end{aligned}
$$
是一个由参数向量决定的函数族。

#### 1.3.2 策略

​	如何从假设空间中选取最优模型。

##### 1.3.2.1 损失函数与风险函数

- **损失函数**：度量模型一次预测的好坏.
- **风险函数**：度量平均意义下模型预测的好坏.

用损失函数（loss function）或代价函数（cost function）来**度量预测错误的程度**。

损失函数是 $f(X)$ 和 $Y$ 的**非负实值函数**，记作 $L(Y,f(X))$.

常见损失函数：

1. 0-1 损失函数

$$
L(Y,f(X))=
\begin{equation}
\left\{
        \begin{array}{lr}
            1, & Y\neq f(X)  \\
            0, & Y=f(X)      \\
        \end{array}
\right.
\end{equation}
$$

2. 平方损失函数 (quadratic)

$$
L(Y,f(X))=(Y-f(X))^2
$$

3. 绝对损失函数 (absolute)

$$
L(Y,f(X))=|Y-f(X)|
$$

4. 对数损失函数 (logarithmic) 或 对数似然损失函数 (log-likelihood)

$$
L(Y,f(X))=-log\ P(Y|X)
$$

损失函数值越小，模型就越好。

风险函数：
$$
R_{exp}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X}\times \mathcal{Y}}L(y,f(x))P(x,y)dxdy
$$
**学习的目标是选择期望风险最小的模型。**由于联合分布 $P(X,Y)$ 是**未知**的，$R_{exp}(f)$ 不能直接计算。实际上如果知道联合分布，也就不需要学习了。所以监督学习就成为一个病态问题。

**经验风险**：给定一个训练数据集 $T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，**模型 $f(X)$ 关于训练数据集的平均损失**称为经验风险（empirical risk）或 经验损失（empirical loss），记作 $R_{emp}$：
$$
R_{emp}(f)=\frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))
$$
**期望风险 $R_{exp}(f)$ 是模型关于联合分布的期望损失，经验风险 $R_{emp}(f)$ 是模型关于训练样本集的平均损失**。根据大数定律，当样本容量 $N \to \infty$ 时，经验风险趋于期望风险，即 $R_{emp}(f)\to R_{exp}(f)$. 所以一个很自然的想法是**用经验风险估计期望风险**。但是由于训练样本数有限，估计结果不理想，要对期望风险进行一定的**矫正**。这关系到两个策略：经验风险最小化与结构风险最小化。

##### 1.3.2.2 经验风险最小化与结构风险最小化

**经验风险最小化**(empirical risk minimization, ERM)的策略认为：经验风险最小的模型就是最优的模型，也就是求解最优化问题：
$$
min_{f\in\mathcal{F}}\frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i))
$$
当样本容量足够大时，结果较好。

> 比如：当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于**极大似然估计**。

但是，当样本容量**很小**时，会产生 “过拟合（over-fitting）” 现象。

**结构风险最小化**（structural risk minimization, SRM）是为了防止过拟合。在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）。结构风险定义为：
$$
R_{srm}=\lambda J(f) + \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))
$$
其中 $J(f)$ 为模型的复杂度，是定义在假设空间 $\mathcal{F}$ 上的泛函。$\lambda \ge 0$ 是系数，用以权衡经验风险和模型复杂度。

> 比如：贝叶斯估计中的最大后验概率估计。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验估计。
>
> （注：没看出来贝叶斯有正则化项？？？）

结构风险最小化也就是求解最优化问题：
$$
min_{f\in\mathcal{F}}\lambda J(f) + \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))
$$
这样，监督学习就变成了经验风险或结构风险的最优化问题。经验风险函数或结构风险函数就是最优化的目标函数（object function）。

#### 1.3.3 算法





### 1.4 模型评估与模型选择



### 1.5 正则化与交叉验证





### 1.6 泛化能力





### 1.7 生成模型与判别模型





### 1.8 分类问题



### 1.9 标注问题



### 1.10 回归问题



### 本章概要







<a id="2"></a>

## Ch02









<a id="3"></a>

## Ch03







<a id="4"></a>

## Ch04 朴素贝叶斯法

### 4.1 朴素贝叶斯法的学习与分类

#### 4.1.1 基本方法

输入空间 $\mathcal{X}\subseteq \pmb R^n$，输入特征向量 $x\in \mathcal{X}$

输出空间 $\mathcal{Y}=\{c_1,c_2,...,c_K\}$，输出类标记 $y\in \mathcal{Y}$

先验概率分布 $P(Y=c_k),\quad k=1,2,...,K$

条件概率分布 $P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),\quad k=1,2,...,K$

**条件独立性假设：**
$$
\begin{aligned}
P(X=x|Y=c_k)&
= P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\&
= \prod_{j=1}^{n}P(X^{(n)}=x^{(n)}|Y=c_k)
\end{aligned}
$$
这相当于说：

​        **用于分类的特征在类确定的条件下都是条件独立的.**

朴素贝叶斯分类器：
$$
\begin{aligned}
y=f(x)&
= argmax_{c_k} P(Y=c_k|X=x)\\&
= argmax_{c_k} \frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}\\&
= argmax_{c_k} P(X=x|Y=c_k)P(Y=c_k)\\&
= argmax_{c_k} P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)
\end{aligned}
$$

#### 4.1.2 后验概率最大化的意义

**朴素贝叶斯法将实例分到后验概率中，这等价于期望风险最小化。**

假设选择0-1损失函数：
$$
L(Y,f(X))=
\begin{equation}
\left\{
        \begin{array}{lr}
            1, & Y\neq f(X)  \\
            0, & Y=f(X)      \\
        \end{array}
\right.
\end{equation}
$$
取条件期望
$$
\begin{aligned}
R_{exp}(f)&
= E(L(Y,f(x))|X=x)\\&
= \sum_{k=1}^K L(c_k,f(x))P(Y=c_k|X=x)\\&
= 1 - P(Y=f(x)|X=x)\\
f(x)&
= argmin_{f(x)}\ R_{exp}(f)\\&
= argmax_{c_k}\ P(Y=c_k|X=x)
\end{aligned}
$$
即后验概率最大化准则。（注：这里书上的推导略奇怪）



### 4.2 朴素贝叶斯法的参数估计

#### 4.2.1 极大似然估计

条件概率 $P(Y=c_k)$ 的极大似然估计是：
$$
P(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N},\quad k=1,2,...,K
$$
设第 $j$ 个特征 $x^{(j)}$ 可能的取值集合为 $\{a_{j1},a_{j2},...,a_{jS_j}\}$，条件概率 $P(X^{(j)}=a_{jl}|Y=c_k)$ 的极大似然估计是：
$$
\begin{aligned}
& P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}\\
& j=1,2,...,n;\quad l=1,2,...,S_j;\quad k=1,2,...,K
\end{aligned}
$$

#### 4.2.2 学习与分类算法

**算法 4.1** (**朴素贝叶斯算法**(**naïve Bayes algorithm**))

​    输入：训练数据 $T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$ ，其中 $x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$，$x_i^{(j)}$ 是第$i$个样本的第$j$个特征，$x_i^{(j)}\in\{a_{j1},a_{j2},...,a_{jS_j}\}$ ，$a_{jl}$ 是第$j$个特征的可能的第$l$个值，$j=1,2,...,n,\quad l=1,2,...,S_j,\quad y_i\in \{c_1,c_2,...,c_K\};$ 实例 $x;$

​    输出：实例 $x$ 的分类。

(1) 计算先验概率及条件概率
$$
\begin{aligned}
& P(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N},\quad k=1,2,...,K     \\
& P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}                                          \\
& j=1,2,...,n;\quad l=1,2,...,S_j;\quad k=1,2,...,K
\end{aligned}
$$
(2) 对于给定的实例 $x=(x^{(1)},x^{(2)},...,x^{(n)})^T$，计算
$$
P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
$$
(3) 确定实例 $x$ 的类
$$
y=argmax_{c_k} P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
$$

#### 4.2.3 贝叶斯估计

用极大似然估计可能出现0，这会影响到之后的计算。

因此做一些修正，使用 **贝叶斯估计**。

条件概率的贝叶斯估计是
$$
\begin{aligned}
& P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\lambda + \sum_{i=1}^N I(x^{(j)}=a_{jl},y_i=c_k)}{S_j\lambda + \sum_{i=1}^N I(y_i=c_k)}    \\
& P_\lambda(Y=c_k)=\frac{\lambda + \sum_{i=1}^N I(y_i=c_k)}{N+K\lambda}
\end{aligned}
$$
可以证明这依然是概率分布，略。

式中 $\lambda\ge 0$，**等价于在随机变量各个取值的频数上赋予一个正数 $\lambda>0$**

**注：**

1. 通过贝叶斯估计处理之后的频数是**无法还原**成真实数据的（**除非**随机变量的每个特征的取值集合拥有相同的基数），即不存在真实的数据，由其计算而来的频数如此分布。（possible risk???）
2. 其实就是统计完频数之后发现有些地方为0或差距太大，因此做一些**平滑处理**，如果觉得统一加一个数效果不好，那可以采用其他的平滑，可以参考数字图像处理中的一些技术。（不过当然要满足概率分布）

### 本章概要

1. 概率估计方法：极大似然估计，贝叶斯估计
2. 一个较强的假设：条件独立
3. 后验概率最大 <=> 0-1损失函数时的期望风险最小化





<a id="5"></a>

## Ch05









<a id="6"></a>

## Ch06







<a id="7"></a>

## Ch07







<a id="8"></a>

## Ch08







<a id="9"></a>

## Ch09

















