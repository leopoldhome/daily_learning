# 统计学习方法笔记



<a id="1"></a>

## Ch01







<a id="2"></a>

## Ch02









<a id="3"></a>

## Ch03







<a id="4"></a>

## Ch04 朴素贝叶斯法

### 4.1 朴素贝叶斯法的学习与分类

#### 4.1.1 基本方法

输入空间 $\mathcal{X}\subseteq \pmb R^n$，输入特征向量 $x\in \mathcal{X}$

输出空间 $\mathcal{Y}=\{c_1,c_2,...,c_K\}$，输出类标记 $y\in \mathcal{Y}$

先验概率分布 $P(Y=c_k),\quad k=1,2,...,K$

条件概率分布 $P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),\quad k=1,2,...,K$

**条件独立性假设：**
$$
\begin{aligned}
P(X=x|Y=c_k)&
= P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\&
= \prod_{j=1}^{n}P(X^{(n)}=x^{(n)}|Y=c_k)
\end{aligned}
$$
这相当于说：

​        **用于分类的特征在类确定的条件下都是条件独立的.**

朴素贝叶斯分类器：
$$
\begin{aligned}
y=f(x)&
= argmax_{c_k} P(Y=c_k|X=x)\\&
= argmax_{c_k} \frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}\\&
= argmax_{c_k} P(X=x|Y=c_k)P(Y=c_k)\\&
= argmax_{c_k} P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)
\end{aligned}
$$

#### 4.1.2 后验概率最大化的意义

**朴素贝叶斯法将实例分到后验概率中，这等价于期望风险最小化。**

假设选择0-1损失函数：
$$
L(Y,f(X))=
\begin{equation}
\left\{
        \begin{array}{lr}
            1, & Y\neq f(X)  \\
            0, & Y=f(X)      \\
        \end{array}
\right.
\end{equation}
$$
取条件期望
$$
\begin{aligned}
R_{exp}(f)&
= E(L(Y,f(x))|X=x)\\&
= \sum_{k=1}^K L(c_k,f(x))P(Y=c_k|X=x)\\&
= 1 - P(Y=f(x)|X=x)\\
f(x)&
= argmin_{f(x)}\ R_{exp}(f)\\&
= argmax_{c_k}\ P(Y=c_k|X=x)
\end{aligned}
$$
即后验概率最大化准则。（注：这里书上的推导略奇怪）



### 4.2 朴素贝叶斯法的参数估计

#### 4.2.1 极大似然估计

条件概率 $P(Y=c_k)$ 的极大似然估计是：
$$
P(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N},\quad k=1,2,...,K
$$
设第 $j$ 个特征 $x^{(j)}$ 可能的取值集合为 $\{a_{j1},a_{j2},...,a_{jS_j}\}$，条件概率 $P(X^{(j)}=a_{jl}|Y=c_k)$ 的极大似然估计是：
$$
\begin{aligned}
& P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}\\
& j=1,2,...,n;\quad l=1,2,...,S_j;\quad k=1,2,...,K
\end{aligned}
$$

#### 4.2.2 学习与分类算法

**算法 4.1** (**朴素贝叶斯算法**(**naïve Bayes algorithm**))

​    输入：训练数据 $T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$ ，其中 $x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$，$x_i^{(j)}$ 是第$i$个样本的第$j$个特征，$x_i^{(j)}\in\{a_{j1},a_{j2},...,a_{jS_j}\}$ ，$a_{jl}$ 是第$j$个特征的可能的第$l$个值，$j=1,2,...,n,\quad l=1,2,...,S_j,\quad y_i\in \{c_1,c_2,...,c_K\};$ 实例 $x;$

​    输出：实例 $x$ 的分类。

(1) 计算先验概率及条件概率
$$
\begin{aligned}
& P(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N},\quad k=1,2,...,K     \\
& P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}                                          \\
& j=1,2,...,n;\quad l=1,2,...,S_j;\quad k=1,2,...,K
\end{aligned}
$$
(2) 对于给定的实例 $x=(x^{(1)},x^{(2)},...,x^{(n)})^T$，计算
$$
P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
$$
(3) 确定实例 $x$ 的类
$$
y=argmax_{c_k} P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
$$

#### 4.2.3 贝叶斯估计

用极大似然估计可能出现0，这会影响到之后的计算。

因此做一些修正，使用 **贝叶斯估计**。

条件概率的贝叶斯估计是
$$
\begin{aligned}
& P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\lambda + \sum_{i=1}^N I(x^{(j)}=a_{jl},y_i=c_k)}{S_j\lambda + \sum_{i=1}^N I(y_i=c_k)}    \\
& P_\lambda(Y=c_k)=\frac{\lambda + \sum_{i=1}^N I(y_i=c_k)}{N+K\lambda}
\end{aligned}
$$
可以证明这依然是概率分布，略。

式中 $\lambda\ge 0$，**等价于在随机变量各个取值的频数上赋予一个正数 $\lambda>0$**

**注：**

1. 通过贝叶斯估计处理之后的频数是**无法还原**成真实数据的（**除非**随机变量的每个特征的取值集合拥有相同的基数），即不存在真实的数据，由其计算而来的频数如此分布。（possible risk???）
2. 其实就是统计完频数之后发现有些地方为0或差距太大，因此做一些**平滑处理**，如果觉得统一加一个数效果不好，那可以采用其他的平滑，可以参考数字图像处理中的一些技术。（不过当然要满足概率分布）

### 本章概要

1. 概率估计方法：极大似然估计，贝叶斯估计
2. 一个较强的假设：条件独立
3. 后验概率最大 <=> 0-1损失函数时的期望风险最小化





<a id="5"></a>

## Ch05









<a id="6"></a>

## Ch06







<a id="7"></a>

## Ch07







<a id="8"></a>

## Ch08







<a id="9"></a>

## Ch09

















