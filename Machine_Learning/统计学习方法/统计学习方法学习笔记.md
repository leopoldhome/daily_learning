# 统计学习方法笔记



<a id="1"></a>

## Ch01







<a id="2"></a>

## Ch02









<a id="3"></a>

## Ch03







<a id="4"></a>

## Ch04 朴素贝叶斯法

### 4.1 朴素贝叶斯法的学习与分类

#### 4.1.1 基本方法

输入空间 $\mathcal{X}\subseteq \pmb R^n$，输入特征向量 $x\in \mathcal{X}$

输出空间 $\mathcal{Y}=\{c_1,c_2,...,c_K\}$，输出类标记 $y\in \mathcal{Y}$

先验概率分布 $P(Y=c_k),\quad k=1,2,...,K$

条件概率分布 $P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),\quad k=1,2,...,K$

**条件独立性假设：**
$$
\begin{aligned}
P(X=x|Y=c_k)&
= P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\&
= \prod_{j=1}^{n}P(X^{(n)}=x^{(n)}|Y=c_k)
\end{aligned}
$$
这相当于说：

​        **用于分类的特征在类确定的条件下都是条件独立的.**

朴素贝叶斯分类器：
$$
\begin{aligned}
y=f(x)&
= argmax_{c_k} P(Y=c_k|X=x)\\&
= argmax_{c_k} \frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}\\&
= argmax_{c_k} P(X=x|Y=c_k)P(Y=c_k)\\&
= argmax_{c_k} P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)
\end{aligned}
$$

#### 4.1.2 后验概率最大化的意义

**朴素贝叶斯法将实例分到后验概率中，这等价于期望风险最小化。**

假设选择0-1损失函数：
$$
L(Y,f(X))=
\begin{equation}
\left\{
        \begin{array}{lr}
            1, & Y\neq f(X)  \\
            0, & Y=f(X)      \\
        \end{array}
\right.
\end{equation}
$$
取条件期望
$$
\begin{aligned}
R_{exp}(f)&
= E(L(Y,f(x))|X=x)\\&
= \sum_{k=1}^K L(c_k,f(x))P(Y=c_k|X=x)\\&
= 1 - P(Y=f(x)|X=x)\\
f(x)&
= argmin_{f(x)}\ R_{exp}(f)\\&
= argmax_{c_k}\ P(Y=c_k|X=x)
\end{aligned}
$$
即后验概率最大化准则。



### 4.2 朴素贝叶斯法的参数估计

#### 4.2.1 极大似然估计









<a id="5"></a>

## Ch05









<a id="6"></a>

## Ch06







<a id="7"></a>

## Ch07







<a id="8"></a>

## Ch08







<a id="9"></a>

## Ch09

















